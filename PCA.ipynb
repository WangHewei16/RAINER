{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# data analysis related library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# some visualization related library\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib.colors import LogNorm\n",
    "import seaborn as sns\n",
    "\n",
    "# category_encoders and warning related library\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore')\n",
    "import category_encoders \n",
    "\n",
    "# sklearn related ML library\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, RocCurveDisplay\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "# other useful libray\n",
    "import missingno as msno\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from pylab import subplots_adjust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read Data & Quality Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "weather_data = pd.read_csv(\"weatherAUS.csv\")\n",
    "weather_data.info()\n",
    "weather_data.shape\n",
    "weather_data.head()\n",
    "weather_data.describe()\n",
    "weather_data.isna().sum().sum()\n",
    "weather_data.isnull().sum().sort_values(ascending=False)\n",
    "(weather_data.isnull().sum() / weather_data.isnull().count()).sort_values(ascending=False)\n",
    "msno.dendrogram(weather_data)\n",
    "plt.show()\n",
    "msno.heatmap(weather_data)\n",
    "plt.show()\n",
    "msno.matrix(weather_data,color=(47/255,127/255,255/255))\n",
    "plt.show()\n",
    "msno.bar(weather_data.sample(1000), color=(255/255,151/225,0/255))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'weatherAUS.csv'\n",
    "weather_data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop columns with a high amount of missing values\n",
    "weather_data.drop([\"Evaporation\", \"Sunshine\", \"Cloud9am\", \"Cloud3pm\"], inplace=True, axis=1)\n",
    "\n",
    "# Handle missing values\n",
    "for col in weather_data.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    weather_data[col] = weather_data[col].fillna(weather_data[col].mean())\n",
    "\n",
    "for col in weather_data.select_dtypes(include=['object']).columns:\n",
    "    weather_data[col] = weather_data[col].fillna(weather_data[col].mode()[0])\n",
    "\n",
    "# Convert categorical features to label classes\n",
    "label_encoder = LabelEncoder()\n",
    "for col in ['Location', 'RainToday', 'RainTomorrow', 'WindDir3pm', 'WindGustDir', 'WindDir9am']:\n",
    "    weather_data[col] = label_encoder.fit_transform(weather_data[col])\n",
    "\n",
    "# Drop the 'Date' column as it is non-numeric and causing issues\n",
    "weather_data = weather_data.drop(['Date'], axis=1)\n",
    "\n",
    "# Standardize the data\n",
    "features = weather_data.drop(['RainTomorrow'], axis=1)\n",
    "target = weather_data['RainTomorrow']\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Apply PCA for the original dataset\n",
    "pca_original = PCA()\n",
    "principal_components_original = pca_original.fit_transform(features_scaled)\n",
    "\n",
    "# Scree plot for original and balanced datasets side by side\n",
    "explained_variance_original = pca_original.explained_variance_ratio_ * 100\n",
    "\n",
    "# Upsample the data for balancing classes\n",
    "normal_weather_data = weather_data[weather_data.RainTomorrow == 0]\n",
    "abnormal_weather_data = weather_data[weather_data.RainTomorrow == 1]\n",
    "upsampled_data = resample(abnormal_weather_data, replace=True, random_state=123, n_samples=len(normal_weather_data))\n",
    "weather_data_balanced = pd.concat([normal_weather_data, upsampled_data])\n",
    "\n",
    "# Standardize the balanced data\n",
    "features_balanced = weather_data_balanced.drop(['RainTomorrow'], axis=1)\n",
    "target_balanced = weather_data_balanced['RainTomorrow']\n",
    "features_balanced_scaled = scaler.fit_transform(features_balanced)\n",
    "\n",
    "# Apply PCA for balanced dataset\n",
    "pca_balanced = PCA()\n",
    "principal_components_balanced = pca_balanced.fit_transform(features_balanced_scaled)\n",
    "explained_variance_balanced = pca_balanced.explained_variance_ratio_ * 100\n",
    "\n",
    "# Plotting Scree plots side by side\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x=np.arange(1, len(explained_variance_original) + 1), y=explained_variance_original, color='b')\n",
    "plt.plot(np.arange(1, len(explained_variance_original) + 1), explained_variance_original, 'ko-')\n",
    "plt.xlabel('Dimensions')\n",
    "plt.ylabel('Percentage of explained variance')\n",
    "plt.title('Scree Plot - Original Dataset')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(x=np.arange(1, len(explained_variance_balanced) + 1), y=explained_variance_balanced, color='b')\n",
    "plt.plot(np.arange(1, len(explained_variance_balanced) + 1), explained_variance_balanced, 'ko-')\n",
    "plt.xlabel('Dimensions')\n",
    "plt.ylabel('Percentage of explained variance')\n",
    "plt.title('Scree Plot - Balanced Dataset')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print contribution of features to the first two principal components for original dataset\n",
    "pca_2d_original = PCA(n_components=2)\n",
    "principal_components_2d_original = pca_2d_original.fit_transform(features_scaled)\n",
    "pca_components_original = pca_2d_original.components_\n",
    "contribution_df_original = pd.DataFrame(pca_components_original.T, columns=['PC1', 'PC2'], index=features.columns)\n",
    "print(\"Contribution of features to the first two principal components (Original Dataset):\")\n",
    "print(contribution_df_original)\n",
    "\n",
    "# Print contribution of features to the first two principal components for balanced dataset\n",
    "pca_2d_balanced = PCA(n_components=2)\n",
    "principal_components_2d_balanced = pca_2d_balanced.fit_transform(features_balanced_scaled)\n",
    "pca_components_balanced = pca_2d_balanced.components_\n",
    "contribution_df_balanced = pd.DataFrame(pca_components_balanced.T, columns=['PC1', 'PC2'], index=features.columns)\n",
    "print(\"Contribution of features to the first two principal components (Balanced Dataset):\")\n",
    "print(contribution_df_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "import numpy as np\n",
    "\n",
    "cmap = plt.cm.plasma  \n",
    "\n",
    "def draw_arrow(ax, start, end, color):\n",
    "    arrow = FancyArrowPatch(posA=start, posB=end, arrowstyle='->',\n",
    "                            color=color, linewidth=1.5, mutation_scale=15)\n",
    "    ax.add_artist(arrow)\n",
    "\n",
    "# Biplot for Original Dataset\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "arrow_lengths_original = np.sqrt(np.sum(pca_2d_original.components_**2, axis=0))\n",
    "max_arrow_length_original = np.max(arrow_lengths_original)\n",
    "scaling_factor_original = 0.9 / max_arrow_length_original \n",
    "\n",
    "contributions_original = np.sqrt(np.sum(pca_2d_original.components_**2, axis=0))\n",
    "contribution_normalized = 20 * (contributions_original - np.min(contributions_original)) / (np.max(contributions_original) - np.min(contributions_original))\n",
    "\n",
    "for i, feature in enumerate(features.columns):\n",
    "\n",
    "    contribution = contribution_normalized[i]\n",
    "    scaled_x = pca_2d_original.components_[0, i] * scaling_factor_original\n",
    "    scaled_y = pca_2d_original.components_[1, i] * scaling_factor_original\n",
    "    color = cmap(contribution / 25)  \n",
    "\n",
    "    draw_arrow(ax, (0, 0), (scaled_x, scaled_y), color=color)\n",
    "\n",
    "    offset_x = -0.08 * np.sign(scaled_x)\n",
    "    offset_y = -0.08 * np.sign(scaled_y)\n",
    "    plt.text(scaled_x * 1.15 + offset_x, scaled_y * 1.15 + offset_y, feature,\n",
    "             color=color, fontsize=9, ha='center', va='center', fontweight='medium', zorder=10)\n",
    "\n",
    "plt.xlabel(f'Dim1 ({pca_2d_original.explained_variance_ratio_[0] * 100:.1f}%)', fontsize=12)\n",
    "plt.ylabel(f'Dim2 ({pca_2d_original.explained_variance_ratio_[1] * 100:.1f}%)', fontsize=12)\n",
    "plt.title('Variables - PCA', fontsize=14)\n",
    "\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=0.6)\n",
    "plt.axvline(0, color='black', linestyle='--', linewidth=0.6)\n",
    "plt.gca().add_patch(plt.Circle((0, 0), 1, color='black', fill=False, linestyle='--', linewidth=0.6))\n",
    "plt.gca().set_aspect('equal', adjustable='datalim')\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(-1, 1)\n",
    "\n",
    "plt.grid(True, color='lightgray', linestyle='-', linewidth=0.5)\n",
    "\n",
    "\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=0, vmax=20))  # 修改了vmin和vmax\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, ax=plt.gca(), orientation='vertical', fraction=0.03, pad=0.04)\n",
    "cbar.set_label('contrib', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Biplot for Balanced Dataset\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "arrow_lengths_balanced = np.sqrt(np.sum(pca_2d_balanced.components_**2, axis=0))\n",
    "max_arrow_length_balanced = np.max(arrow_lengths_balanced)\n",
    "scaling_factor_balanced = 0.9 / max_arrow_length_balanced\n",
    "\n",
    "\n",
    "contributions_balanced = np.sqrt(np.sum(pca_2d_balanced.components_**2, axis=0))\n",
    "\n",
    "contribution_normalized_balanced = 20 * (contributions_balanced - np.min(contributions_balanced)) / (np.max(contributions_balanced) - np.min(contributions_balanced))\n",
    "\n",
    "for i, feature in enumerate(features.columns):\n",
    "\n",
    "    contribution = contribution_normalized_balanced[i]\n",
    "    scaled_x = pca_2d_balanced.components_[0, i] * scaling_factor_balanced\n",
    "    scaled_y = pca_2d_balanced.components_[1, i] * scaling_factor_balanced\n",
    "    color = cmap(contribution / 25)  \n",
    "\n",
    "    draw_arrow(ax, (0, 0), (scaled_x, scaled_y), color=color)\n",
    "\n",
    "    offset_x = -0.1 * np.sign(scaled_x)\n",
    "    offset_y = -0.05 * np.sign(scaled_y)\n",
    "    plt.text(scaled_x * 1.15 + offset_x, scaled_y * 1.15 + offset_y, feature,\n",
    "             color=color, fontsize=9, ha='center', va='center', fontweight='medium', zorder=10)\n",
    "\n",
    "plt.xlabel(f'Dim1 ({pca_2d_balanced.explained_variance_ratio_[0] * 100:.1f}%)', fontsize=12)\n",
    "plt.ylabel(f'Dim2 ({pca_2d_balanced.explained_variance_ratio_[1] * 100:.1f}%)', fontsize=12)\n",
    "plt.title('Variables - PCA', fontsize=14)\n",
    "\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=0.6)\n",
    "plt.axvline(0, color='black', linestyle='--', linewidth=0.6)\n",
    "plt.gca().add_patch(plt.Circle((0, 0), 1, color='black', fill=False, linestyle='--', linewidth=0.6))\n",
    "plt.gca().set_aspect('equal', adjustable='datalim')\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(-1, 1)\n",
    "\n",
    "plt.grid(True, color='lightgray', linestyle='-', linewidth=0.5)\n",
    "\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=0, vmax=20))  # 修改了vmin和vmax\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, ax=plt.gca(), orientation='vertical', fraction=0.03, pad=0.04)\n",
    "cbar.set_label('contrib', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "file_path = 'weatherAUS.csv'\n",
    "weather_data = pd.read_csv(file_path)\n",
    "\n",
    "weather_data.drop([\"Evaporation\", \"Sunshine\", \"Cloud9am\", \"Cloud3pm\"], inplace=True, axis=1)\n",
    "\n",
    "for col in weather_data.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    weather_data[col] = weather_data[col].fillna(weather_data[col].mean())\n",
    "\n",
    "for col in weather_data.select_dtypes(include=['object']).columns:\n",
    "    weather_data[col] = weather_data[col].fillna(weather_data[col].mode()[0])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "for col in ['Location', 'RainToday', 'RainTomorrow', 'WindDir3pm', 'WindGustDir', 'WindDir9am']:\n",
    "    weather_data[col] = label_encoder.fit_transform(weather_data[col])\n",
    "\n",
    "weather_data = weather_data.drop(['Date'], axis=1)\n",
    "\n",
    "features = weather_data.drop(['RainTomorrow'], axis=1)\n",
    "target = weather_data['RainTomorrow']\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "pca_original = PCA(n_components=2)\n",
    "principal_components_original = pca_original.fit_transform(features_scaled)\n",
    "cos2_original = (principal_components_original ** 2).sum(axis=1) / (principal_components_original ** 2).sum()\n",
    "\n",
    "normal_weather_data = weather_data[weather_data.RainTomorrow == 0]\n",
    "abnormal_weather_data = weather_data[weather_data.RainTomorrow == 1]\n",
    "upsampled_data = resample(abnormal_weather_data, replace=True, random_state=123, n_samples=len(normal_weather_data))\n",
    "weather_data_balanced = pd.concat([normal_weather_data, upsampled_data])\n",
    "\n",
    "features_balanced = weather_data_balanced.drop(['RainTomorrow'], axis=1)\n",
    "target_balanced = weather_data_balanced['RainTomorrow']\n",
    "features_balanced_scaled = scaler.fit_transform(features_balanced)\n",
    "\n",
    "pca_balanced = PCA(n_components=2)\n",
    "principal_components_balanced = pca_balanced.fit_transform(features_balanced_scaled)\n",
    "cos2_balanced = (principal_components_balanced ** 2).sum(axis=1) / (principal_components_balanced ** 2).sum()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "# ----------------------------\n",
    "# 1：cos² \n",
    "ax1 = axes[0, 0]\n",
    "ax1.set_facecolor('white')  \n",
    "scatter1 = sns.scatterplot(\n",
    "    x=principal_components_original[:, 0],\n",
    "    y=principal_components_original[:, 1],\n",
    "    hue=cos2_original,\n",
    "    palette='RdYlBu',\n",
    "    alpha=0.6,\n",
    "    edgecolor=None,\n",
    "    ax=ax1\n",
    ")\n",
    "ax1.set_xlabel(f'Dim1 ({pca_original.explained_variance_ratio_[0] * 100:.1f}%)')\n",
    "ax1.set_ylabel(f'Dim2 ({pca_original.explained_variance_ratio_[1] * 100:.1f}%)')\n",
    "ax1.set_title('Individuals - PCA (Original Dataset)')\n",
    "ax1.axhline(0, linestyle='--', color='black', linewidth=0.5)\n",
    "ax1.axvline(0, linestyle='--', color='black', linewidth=0.5)\n",
    "\n",
    "# adjust cos2_original \n",
    "legend1 = ax1.legend(title='cos²', fontsize=22, title_fontsize=24, markerscale=3, loc='best')\n",
    "for text in legend1.get_texts():\n",
    "    text.set_fontsize(22)\n",
    "legend1.set_title('cos²', prop={'size': 24})\n",
    "\n",
    "# ----------------------------\n",
    "# 2：RainTomorrow\n",
    "ax2 = axes[0, 1]\n",
    "ax2.set_facecolor('white')  \n",
    "scatter2 = sns.scatterplot(\n",
    "    x=principal_components_original[:, 0],\n",
    "    y=principal_components_original[:, 1],\n",
    "    hue=target,\n",
    "    palette='RdYlBu',\n",
    "    alpha=0.6,\n",
    "    edgecolor=None,\n",
    "    style=target,\n",
    "    markers={0: 'o', 1: 'X'},\n",
    "    s=2,\n",
    "    ax=ax2\n",
    ")\n",
    "ax2.set_xlabel(f'Dim1 ({pca_original.explained_variance_ratio_[0] * 100:.1f}%)')\n",
    "ax2.set_ylabel(f'Dim2 ({pca_original.explained_variance_ratio_[1] * 100:.1f}%)')\n",
    "ax2.set_title('Individuals - PCA (Original Dataset, Groups)')\n",
    "ax2.axhline(0, linestyle='--', color='black', linewidth=0.5)\n",
    "ax2.axvline(0, linestyle='--', color='black', linewidth=0.5)\n",
    "\n",
    "# adjust RainTomorrow \n",
    "legend2 = ax2.legend(title='RainTomorrow', fontsize=22, title_fontsize=24, markerscale=15)\n",
    "for text in legend2.get_texts():\n",
    "    text.set_fontsize(22)\n",
    "legend2.set_title('RainTomorrow', prop={'size': 24})\n",
    "\n",
    "# ----------------------------\n",
    "# 3：cos²\n",
    "ax3 = axes[1, 0]\n",
    "ax3.set_facecolor('white')  \n",
    "scatter3 = sns.scatterplot(\n",
    "    x=principal_components_balanced[:, 0],\n",
    "    y=principal_components_balanced[:, 1],\n",
    "    hue=cos2_balanced,\n",
    "    palette='RdYlBu',\n",
    "    alpha=0.6,\n",
    "    edgecolor=None,\n",
    "    ax=ax3\n",
    ")\n",
    "ax3.set_xlabel(f'Dim1 ({pca_balanced.explained_variance_ratio_[0] * 100:.1f}%)')\n",
    "ax3.set_ylabel(f'Dim2 ({pca_balanced.explained_variance_ratio_[1] * 100:.1f}%)')\n",
    "ax3.set_title('Individuals - PCA (Balanced Dataset)')\n",
    "ax3.axhline(0, linestyle='--', color='black', linewidth=0.5)\n",
    "ax3.axvline(0, linestyle='--', color='black', linewidth=0.5)\n",
    "\n",
    "# adjust cos2_balanced \n",
    "legend3 = ax3.legend(title='cos²', fontsize=22, title_fontsize=24, markerscale=3, loc='best')\n",
    "for text in legend3.get_texts():\n",
    "    text.set_fontsize(22)\n",
    "legend3.set_title('cos²', prop={'size': 24})\n",
    "\n",
    "# ----------------------------\n",
    "# 4：RainTomorrow\n",
    "ax4 = axes[1, 1]\n",
    "ax4.set_facecolor('white')  \n",
    "scatter4 = sns.scatterplot(\n",
    "    x=principal_components_balanced[:, 0],\n",
    "    y=principal_components_balanced[:, 1],\n",
    "    hue=target_balanced,\n",
    "    palette='RdYlBu',\n",
    "    alpha=0.6,\n",
    "    edgecolor=None,\n",
    "    style=target_balanced,\n",
    "    markers={0: 'o', 1: 'X'},\n",
    "    s=2,\n",
    "    ax=ax4\n",
    ")\n",
    "ax4.set_xlabel(f'Dim1 ({pca_balanced.explained_variance_ratio_[0] * 100:.1f}%)')\n",
    "ax4.set_ylabel(f'Dim2 ({pca_balanced.explained_variance_ratio_[1] * 100:.1f}%)')\n",
    "ax4.set_title('Individuals - PCA (Balanced Dataset, Groups)')\n",
    "ax4.axhline(0, linestyle='--', color='black', linewidth=0.5)\n",
    "ax4.axvline(0, linestyle='--', color='black', linewidth=0.5)\n",
    "\n",
    "# adjust RainTomorrow_balanced \n",
    "legend4 = ax4.legend(title='RainTomorrow', fontsize=22, title_fontsize=24, markerscale=15)\n",
    "for text in legend4.get_texts():\n",
    "    text.set_fontsize(22)\n",
    "legend4.set_title('RainTomorrow', prop={'size': 24})\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig('pca_plots.png', transparent=True)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
